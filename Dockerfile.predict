# build llama cpp
ARG UBUNTU_VERSION=24.04
FROM ubuntu:$UBUNTU_VERSION AS build

RUN apt-get update && \
    apt-get install -y build-essential git cmake libcurl4-openssl-dev

WORKDIR /app
RUN git clone --depth 1 https://github.com/ggml-org/llama.cpp.git
WORKDIR /app/llama.cpp
RUN cmake -S . -B build -DCMAKE_BUILD_TYPE=Release -DGGML_NATIVE=OFF -DGGML_BACKEND_DL=ON -DGGML_CPU_ALL_VARIANTS=ON \
    && cmake --build build -j $(nproc)

RUN mkdir -p /app/lib && \
    find build -name "*.so" -exec cp {} /app/lib \;
    
RUN mkdir -p /app/full \
    && cp build/bin/* /app/full \
    && cp *.py /app/full \
    && cp -r gguf-py /app/full \
    && cp -r requirements /app/full \
    && cp requirements.txt /app/full \
    && cp .devops/tools.sh /app/full/tools.sh

RUN mkdir -p /app/lib && \
    find build -name "*.so" -exec cp {} /app/lib \;


FROM ubuntu:$UBUNTU_VERSION AS base
RUN apt-get update \
    && apt-get install -y libgomp1 curl python3 python3-pip \
    && apt autoremove -y \
    && apt clean -y \
    && rm -rf /tmp/* /var/tmp/* \
    && find /var/cache/apt/archives /var/lib/apt/lists -not -name lock -type f -delete \
    && find /var/cache -type f -delete

WORKDIR /app
COPY --from=build /app/lib/ /app
COPY --from=build /app/full/llama-server /app

RUN mkdir -p /home/admin/predict && mkdir -p /home/admin/workspace && mkdir -p /home/admin/data
RUN pip3 install --no-cache-dir --break-system-packages requests
COPY ./run.sh /home/admin/predict/run.sh
COPY ./Qwen3-1.7B-Q2_K.gguf /app/models/Qwen3-1.7B-Q2_K.gguf
RUN rm -rf /app/libggml-cpu-sapphirerapids.so
COPY ./predict_demo.py /home/admin/predict/predict_demo.py
ENTRYPOINT ["bash"]